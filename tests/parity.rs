use std::fs;
use std::path::Path;

use polars::prelude::{
    DataFrame as PlDataFrame, Expr, NamedFrom, PolarsError, Series, col, lit,
};
use robin_sparkless::{DataFrame, SparkSession};
use serde::Deserialize;
use serde_json::Value;

/// Top-level fixture structure, matching the JSON we’ll generate from PySpark.
#[derive(Debug, Deserialize)]
struct Fixture {
    name: String,
    #[allow(dead_code)]
    pyspark_version: Option<String>,
    input: InputSection,
    operations: Vec<Operation>,
    expected: ExpectedSection,
}

#[derive(Debug, Deserialize)]
struct InputSection {
    schema: Vec<ColumnSpec>,
    rows: Vec<Vec<Value>>,
}

#[derive(Debug, Deserialize)]
struct ColumnSpec {
    name: String,
    r#type: String,
}

#[derive(Debug, Deserialize)]
struct ExpectedSection {
    schema: Vec<ColumnSpec>,
    rows: Vec<Vec<Value>>,
}

/// Supported operations in the first parity slice.
#[derive(Debug, Deserialize)]
#[serde(tag = "op")]
enum Operation {
    #[serde(rename = "filter")]
    Filter { expr: String },
    #[serde(rename = "select")]
    Select { columns: Vec<String> },
    #[serde(rename = "orderBy")]
    OrderBy { columns: Vec<String>, #[serde(default)] ascending: Vec<bool> },
    #[serde(rename = "groupBy")]
    GroupBy { columns: Vec<String> },
    #[serde(rename = "agg")]
    Agg { aggregations: Vec<AggregationSpec> },
}

#[derive(Debug, Deserialize)]
struct AggregationSpec {
    func: String,
    alias: String,
}

/// Parity tests generated from PySpark fixtures.
///
/// This test reads JSON fixtures generated by `tests/gen_pyspark_cases.py` and
/// verifies that robin-sparkless produces the same results as PySpark.
#[test]
fn pyspark_parity_fixtures() {
    let fixtures_dir = Path::new("tests/fixtures");
    if !fixtures_dir.exists() {
        // Nothing to run yet; treat as a no-op.
        return;
    }

    for entry in fs::read_dir(fixtures_dir).expect("read fixtures directory") {
        let path = entry.expect("dir entry").path();
        if path.extension().and_then(|s| s.to_str()) != Some("json") {
            continue;
        }

        let text = fs::read_to_string(&path).expect("read fixture");
        let fixture: Fixture =
            serde_json::from_str(&text).expect("parse fixture json");

        // Run the full parity test: create DataFrame, apply operations, compare results
        run_fixture(&fixture).unwrap();
    }
}

fn run_fixture(fixture: &Fixture) -> Result<(), PolarsError> {
    // Basic shape sanity.
    assert!(
        !fixture.input.schema.is_empty(),
        "fixture {} has empty schema",
        fixture.name
    );
    assert_eq!(
        fixture.expected.schema.len(),
        fixture
            .expected
            .rows
            .first()
            .map(|r| r.len())
            .unwrap_or(0),
        "fixture {} expected schema/row length mismatch",
        fixture.name
    );

    // Create SparkSession and DataFrame from input
    let spark = SparkSession::builder().app_name("parity_test").get_or_create();
    let df = create_df_from_input(&spark, &fixture.input)?;

    // Apply operations
    let result_df = apply_operations(df, &fixture.operations)?;

    // Collect and compare results
    let (actual_schema, actual_rows) = collect_to_simple_format(&result_df)?;
    
    // Check if operations include orderBy (for comparison strategy)
    let has_order_by = fixture.operations.iter().any(|op| matches!(op, Operation::OrderBy { .. }));
    
    assert_schema_eq(&actual_schema, &fixture.expected.schema, &fixture.name)?;
    assert_rows_eq(&actual_rows, &fixture.expected.rows, has_order_by, &fixture.name)?;

    Ok(())
}

/// Build a DataFrame from the JSON input section using SparkSession::create_dataframe.
///
/// For the first parity slice we support only a small subset of types:
/// - `int` / `bigint` → `i64`
/// - `string`         → UTF-8
fn create_df_from_input(
    spark: &SparkSession,
    input: &InputSection,
) -> Result<DataFrame, PolarsError> {
    // Convert input to (i64, i64, String) tuples for create_dataframe
    // This assumes the first two columns are int-like and third is string
    if input.schema.len() != 3 {
        // Fallback to direct Polars construction for non-3-column cases
        return create_df_from_input_direct(input);
    }

    let mut tuples: Vec<(i64, i64, String)> = Vec::new();
    for row in &input.rows {
        let v0 = row.get(0).and_then(|v| v.as_i64()).unwrap_or(0);
        let v1 = row.get(1).and_then(|v| v.as_i64()).unwrap_or(0);
        let v2 = row
            .get(2)
            .and_then(|v| v.as_str())
            .map(|s| s.to_string())
            .unwrap_or_else(|| "".to_string());
        tuples.push((v0, v1, v2));
    }

    let col_names: Vec<&str> = input.schema.iter().map(|s| s.name.as_str()).collect();
    spark.create_dataframe(tuples, col_names)
}

/// Fallback: Build a Polars-backed `DataFrame` directly from the JSON input section.
fn create_df_from_input_direct(input: &InputSection) -> Result<DataFrame, PolarsError> {
    let mut cols: Vec<Series> = Vec::with_capacity(input.schema.len());

    for (col_idx, spec) in input.schema.iter().enumerate() {
        match spec.r#type.as_str() {
            "int" | "bigint" | "long" => {
                let mut vals: Vec<Option<i64>> = Vec::with_capacity(input.rows.len());
                for row in &input.rows {
                    let v = row.get(col_idx).cloned().unwrap_or(Value::Null);
                    let opt = match v {
                        Value::Number(n) => n.as_i64(),
                        Value::Null => None,
                        _ => None,
                    };
                    vals.push(opt);
                }
                cols.push(Series::new(spec.name.clone().into(), vals));
            }
            "string" | "str" | "varchar" => {
                let mut vals: Vec<Option<String>> =
                    Vec::with_capacity(input.rows.len());
                for row in &input.rows {
                    let v = row.get(col_idx).cloned().unwrap_or(Value::Null);
                    let opt = match v {
                        Value::String(s) => Some(s),
                        Value::Null => None,
                        other => Some(other.to_string()),
                    };
                    vals.push(opt);
                }
                cols.push(Series::new(spec.name.clone().into(), vals));
            }
            other => {
                return Err(PolarsError::ComputeError(
                    format!(
                        "unsupported type in test fixture: {} (column '{}')",
                        other, spec.name
                    )
                    .into(),
                ));
            }
        }
    }

    let pl_df = PlDataFrame::new(cols.iter().map(|s| s.clone().into()).collect())?;
    Ok(DataFrame::from_polars(pl_df))
}

/// Apply the first parity-slice operations (filter + select + orderBy + groupBy + agg).
///
/// - `filter` supports very simple expressions of the form:
///   - `col('age') > 30`
///   - `col(\"age\") >= 10`
/// - `select` takes explicit column names.
/// - `orderBy` sorts by columns.
/// - `groupBy` creates a GroupedData (must be followed by `agg`).
/// - `agg` applies aggregations to GroupedData and returns a DataFrame.
fn apply_operations(
    mut df: DataFrame,
    ops: &[Operation],
) -> Result<DataFrame, PolarsError> {
    use robin_sparkless::GroupedData;
    
    let mut grouped: Option<GroupedData> = None;
    
    for op in ops {
        match op {
            Operation::Filter { expr } => {
                // Filter must be applied before grouping
                if grouped.is_some() {
                    return Err(PolarsError::ComputeError(
                        "filter cannot be applied after groupBy".into(),
                    ));
                }
                let predicate = parse_simple_filter_expr(expr).map_err(|e| {
                    PolarsError::ComputeError(
                        format!("failed to parse filter expr '{}': {}", expr, e).into(),
                    )
                })?;
                df = df.filter(predicate)?;
            }
            Operation::Select { columns } => {
                // Select must be applied before grouping
                if grouped.is_some() {
                    return Err(PolarsError::ComputeError(
                        "select cannot be applied after groupBy".into(),
                    ));
                }
                let cols: Vec<&str> = columns.iter().map(|s| s.as_str()).collect();
                df = df.select(cols)?;
            }
            Operation::OrderBy { columns, ascending } => {
                // OrderBy can be applied to DataFrame or after aggregation
                if let Some(ref gd) = grouped {
                    // If we have a grouped data, we need to aggregate first
                    return Err(PolarsError::ComputeError(
                        "orderBy cannot be applied to GroupedData, must aggregate first".into(),
                    ));
                }
                let cols: Vec<&str> = columns.iter().map(|s| s.as_str()).collect();
                df = df.order_by(cols, ascending.clone())?;
            }
            Operation::GroupBy { columns } => {
                if grouped.is_some() {
                    return Err(PolarsError::ComputeError(
                        "nested groupBy not supported".into(),
                    ));
                }
                let cols: Vec<&str> = columns.iter().map(|s| s.as_str()).collect();
                grouped = Some(df.group_by(cols)?);
            }
            Operation::Agg { aggregations } => {
                let gd = grouped.take().ok_or_else(|| {
                    PolarsError::ComputeError("agg requires a preceding groupBy".into())
                })?;
                
                // For now, only support count() aggregation
                if aggregations.len() != 1 || aggregations[0].func != "count" {
                    return Err(PolarsError::ComputeError(
                        format!("only count() aggregation supported, got: {:?}", aggregations).into(),
                    ));
                }
                
                df = gd.count()?;
                grouped = None; // Aggregation consumes the GroupedData
            }
        }
    }
    
    if grouped.is_some() {
        return Err(PolarsError::ComputeError(
            "groupBy must be followed by agg".into(),
        ));
    }
    
    Ok(df)
}

/// Very small parser for expressions like: `col('age') > 30`.
fn parse_simple_filter_expr(src: &str) -> Result<Expr, String> {
    let s = src.trim();

    // Expect something like: col('age') > 30
    let col_start = s.find("col(").ok_or("missing 'col('")?;
    let quote1 = s[col_start..]
        .find(['\'', '"'])
        .ok_or("missing opening quote for column")?
        + col_start;
    let rest = &s[quote1 + 1..];
    let quote2 = rest
        .find(['\'', '"'])
        .ok_or("missing closing quote for column")?
        + quote1
        + 1;

    let col_name = &s[quote1 + 1..quote2];

    // After the closing paren, expect an operator and a literal integer.
    let after_col = &s[quote2 + 1..];
    let tokens: Vec<&str> = after_col.split_whitespace().collect();
    if tokens.len() < 2 {
        return Err("expected operator and literal".to_string());
    }

    let op = tokens[0];
    let lit_str = tokens[1];
    let lit_val: i64 = lit_str
        .parse()
        .map_err(|_| format!("unable to parse literal '{}'", lit_str))?;

    let c = col(col_name);
    let lit_e = lit(lit_val);

    let expr = match op {
        ">" => c.gt(lit_e),
        ">=" => c.gt_eq(lit_e),
        "<" => c.lt(lit_e),
        "<=" => c.lt_eq(lit_e),
        "==" | "=" => c.eq(lit_e),
        "!=" => c.neq(lit_e),
        other => return Err(format!("unsupported operator '{}'", other)),
    };

    Ok(expr)
}

/// Collect a DataFrame to a simple (schema, rows) representation for comparison.
fn collect_to_simple_format(df: &DataFrame) -> Result<(Vec<ColumnSpec>, Vec<Vec<Value>>), PolarsError> {
    let pl_df = df.collect()?;
    let schema = pl_df.schema();
    
    // Build schema
    let col_specs: Vec<ColumnSpec> = schema
        .iter()
        .map(|(name, dtype)| ColumnSpec {
            name: name.to_string(),
            r#type: dtype_to_string(dtype),
        })
        .collect();
    
    // Build rows
    let num_rows = pl_df.height();
    let num_cols = schema.len();
    let mut rows: Vec<Vec<Value>> = Vec::with_capacity(num_rows);
    
    // Extract rows by iterating through each column
    for row_idx in 0..num_rows {
        let mut row: Vec<Value> = Vec::with_capacity(num_cols);
        for col_idx in 0..num_cols {
            let series = pl_df.get_columns().get(col_idx).ok_or_else(|| {
                PolarsError::ComputeError(format!("column index {} out of range", col_idx).into())
            })?;
            let json_val = match series.get(row_idx) {
                Ok(av) => match av {
                    polars::prelude::AnyValue::Null => Value::Null,
                    polars::prelude::AnyValue::Int64(v) => Value::Number(v.into()),
                    polars::prelude::AnyValue::Int32(v) => Value::Number(v.into()),
                    polars::prelude::AnyValue::String(v) => Value::String(v.to_string()),
                    _ => Value::String(format!("{:?}", av)),
                },
                Err(_) => Value::Null,
            };
            row.push(json_val);
        }
        rows.push(row);
    }
    
    Ok((col_specs, rows))
}

/// Convert Polars DataType to a simple string representation.
fn dtype_to_string(dtype: &polars::prelude::DataType) -> String {
    match dtype {
        polars::prelude::DataType::Int64 => "bigint".to_string(),
        polars::prelude::DataType::Int32 => "int".to_string(),
        polars::prelude::DataType::String => "string".to_string(),
        _ => format!("{:?}", dtype),
    }
}

/// Assert that schemas match (column names and types).
fn assert_schema_eq(
    actual: &[ColumnSpec],
    expected: &[ColumnSpec],
    fixture_name: &str,
) -> Result<(), PolarsError> {
    if actual.len() != expected.len() {
        return Err(PolarsError::ComputeError(
            format!(
                "fixture {}: schema length mismatch: actual {} columns, expected {}",
                fixture_name, actual.len(), expected.len()
            )
            .into(),
        ));
    }
    
    for (i, (act, exp)) in actual.iter().zip(expected.iter()).enumerate() {
        if act.name != exp.name {
            return Err(PolarsError::ComputeError(
                format!(
                    "fixture {}: column {} name mismatch: actual '{}', expected '{}'",
                    fixture_name, i, act.name, exp.name
                )
                .into(),
            ));
        }
        // Type comparison is lenient for now (int vs bigint both OK)
        if !types_compatible(&act.r#type, &exp.r#type) {
            return Err(PolarsError::ComputeError(
                format!(
                    "fixture {}: column '{}' type mismatch: actual '{}', expected '{}'",
                    fixture_name, act.name, act.r#type, exp.r#type
                )
                .into(),
            ));
        }
    }
    
    Ok(())
}

/// Check if two type strings are compatible (lenient matching).
fn types_compatible(actual: &str, expected: &str) -> bool {
    if actual == expected {
        return true;
    }
    // Allow int/bigint/long to match
    let int_types = ["int", "bigint", "long"];
    if int_types.contains(&actual) && int_types.contains(&expected) {
        return true;
    }
    // Allow string/str/varchar to match
    let string_types = ["string", "str", "varchar"];
    if string_types.contains(&actual) && string_types.contains(&expected) {
        return true;
    }
    false
}

/// Assert that rows match (with optional ordering requirement).
fn assert_rows_eq(
    actual: &[Vec<Value>],
    expected: &[Vec<Value>],
    ordered: bool,
    fixture_name: &str,
) -> Result<(), PolarsError> {
    if actual.len() != expected.len() {
        return Err(PolarsError::ComputeError(
            format!(
                "fixture {}: row count mismatch: actual {} rows, expected {}",
                fixture_name, actual.len(), expected.len()
            )
            .into(),
        ));
    }
    
    let mut actual_sorted = actual.to_vec();
    let mut expected_sorted = expected.to_vec();
    
    if !ordered {
        // Sort both for comparison (convert rows to comparable format)
        actual_sorted.sort_by(|a, b| compare_rows(a, b));
        expected_sorted.sort_by(|a, b| compare_rows(a, b));
    }
    
    for (i, (act_row, exp_row)) in actual_sorted.iter().zip(expected_sorted.iter()).enumerate() {
        if act_row.len() != exp_row.len() {
            return Err(PolarsError::ComputeError(
                format!(
                    "fixture {}: row {} length mismatch: actual {} values, expected {}",
                    fixture_name, i, act_row.len(), exp_row.len()
                )
                .into(),
            ));
        }
        
        for (j, (act_val, exp_val)) in act_row.iter().zip(exp_row.iter()).enumerate() {
            if !values_equal(act_val, exp_val) {
                return Err(PolarsError::ComputeError(
                    format!(
                        "fixture {}: row {}, column {} mismatch: actual {:?}, expected {:?}",
                        fixture_name, i, j, act_val, exp_val
                    )
                    .into(),
                ));
            }
        }
    }
    
    Ok(())
}

/// Compare two rows for sorting (lexicographic).
fn compare_rows(a: &[Value], b: &[Value]) -> std::cmp::Ordering {
    for (av, bv) in a.iter().zip(b.iter()) {
        let ord = compare_values(av, bv);
        if ord != std::cmp::Ordering::Equal {
            return ord;
        }
    }
    a.len().cmp(&b.len())
}

/// Compare two JSON Values for ordering.
fn compare_values(a: &Value, b: &Value) -> std::cmp::Ordering {
    match (a, b) {
        (Value::Null, Value::Null) => std::cmp::Ordering::Equal,
        (Value::Null, _) => std::cmp::Ordering::Less,
        (_, Value::Null) => std::cmp::Ordering::Greater,
        (Value::Number(n1), Value::Number(n2)) => {
            if let (Some(i1), Some(i2)) = (n1.as_i64(), n2.as_i64()) {
                i1.cmp(&i2)
            } else if let (Some(f1), Some(f2)) = (n1.as_f64(), n2.as_f64()) {
                f1.partial_cmp(&f2).unwrap_or(std::cmp::Ordering::Equal)
            } else {
                n1.to_string().cmp(&n2.to_string())
            }
        }
        (Value::String(s1), Value::String(s2)) => s1.cmp(s2),
        (Value::Bool(b1), Value::Bool(b2)) => b1.cmp(b2),
        _ => format!("{:?}", a).cmp(&format!("{:?}", b)),
    }
}

/// Compare two JSON Values for equality (handles nulls, numbers, strings).
fn values_equal(a: &Value, b: &Value) -> bool {
    match (a, b) {
        (Value::Null, Value::Null) => true,
        (Value::Number(n1), Value::Number(n2)) => {
            // Try to compare as i64 first, then f64
            if let (Some(i1), Some(i2)) = (n1.as_i64(), n2.as_i64()) {
                i1 == i2
            } else if let (Some(f1), Some(f2)) = (n1.as_f64(), n2.as_f64()) {
                // Float comparison with small epsilon
                (f1 - f2).abs() < 1e-10
            } else {
                false
            }
        }
        (Value::String(s1), Value::String(s2)) => s1 == s2,
        (Value::Bool(b1), Value::Bool(b2)) => b1 == b2,
        _ => false,
    }
}

