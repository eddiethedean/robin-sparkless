## Rust-first testing and parity

This project is now **Rust-first**. The core behavior of `SparkSession`,
`DataFrame`, `Column`, and related APIs is validated by Rust tests in
`tests/`:

- `dataframe_core.rs` – basic DataFrame creation, `filter`, `select`,
  column–column comparison semantics, etc.
- `groupby_orderby_core.rs` – `group_by` and `order_by` behavior using
  both string and `Column` arguments.
- `sql_core.rs` – `SparkSession::sql`, temp views, and basic DDL
  (e.g. `DROP TABLE` / `DROP VIEW`).
- `delta_core.rs` – `write_delta` / `read_delta_from_path` round‑trip
  tests when the `delta` feature is enabled.
- `lazy_backend.rs`, `error_handling.rs`, `parity.rs`, and other
  existing Rust tests.

### PySpark parity fixtures

PySpark is still used as an **external reference** via JSON fixtures in
`tests/fixtures/`. These fixtures are generated by Python scripts
under `tests/` and `scripts/`, but PySpark is **not** required at test
runtime:

- `tests/parity.rs` loads fixtures from `tests/fixtures/` and validates
  that `robin-sparkless` matches the recorded behavior.
- Python scripts such as `tests/regenerate_expected_from_pyspark.py`
  and `scripts/extract_pyspark_tests.py` can be run manually to refresh
  or expand fixtures when behavior changes.

### Python bindings and CI

The previous Python package (PyO3 bindings, `pyproject.toml`, and
Python‑focused CI) has been removed from the default workflow. The
remaining Python code is limited to:

- Parity fixture generation scripts in `tests/` and `scripts/`.
- Historical Python tests in `tests/python/`, which are no longer run
  in CI and are being superseded by Rust tests.

New tests should be written in Rust, ideally:

- As focused unit tests against `SparkSession` / `DataFrame` /
  `Column`, and
- Backed by PySpark‑derived fixtures when subtle semantics need to be
  preserved.

