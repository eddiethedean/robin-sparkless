# Parity Status (PySpark vs Robin Sparkless)

This doc is the **living parity matrix** for `robin-sparkless`.

- **Oracle**: PySpark (fixtures generated by `tests/gen_pyspark_cases.py`)
- **Harness**: `tests/parity.rs`
- **Fixtures**: `tests/fixtures/*.json`
- **Sparkless integration**: Robin-sparkless is designed to replace Sparkless's backend. Sparkless has 270+ expected_outputs; a fixture converter can convert those to robin-sparkless format. See [SPARKLESS_INTEGRATION_ANALYSIS.md](SPARKLESS_INTEGRATION_ANALYSIS.md) ¬ß4.

Status as of **February 2026**: **PASSING (149 fixtures; array_distinct, with_curdate_now skipped)**. **Phase 24** ‚úÖ **PARTIALLY COMPLETED**: bit/control/JVM/random implemented (bit_and, bit_or, bit_xor, bit_count, bit_get, bitwise_not/bitwiseNOT; assert_true, raise_error; broadcast, spark_partition_id, input_file_name, monotonically_increasing_id, current_catalog, current_database, current_schema, current_user, user; rand, randn with per-row values when used in with_column/with_columns). Fixture `with_bit_ops` covers bit operations. AES crypto (`aes_decrypt`, `aes_encrypt`, `try_aes_decrypt`) remains deferred (documented in PYSPARK_DIFFERENCES). **Phase 23** ‚úÖ **COMPLETED**: JSON/URL/misc (isin, url_decode, url_encode, json_array_length, parse_url, hash, shift_left, shift_right, version, equal_null, stack); fixtures `with_isin`, `with_url_decode`, `with_url_encode`, `json_array_length_test`, `with_hash`, `with_shift_left`. **Phase 22** ‚úÖ **COMPLETED**: Datetime extensions (curdate, now, localtimestamp, date_diff, dateadd, datepart, extract, date_part, unix_micros, unix_millis, unix_seconds, dayname, weekday, make_timestamp, make_timestamp_ntz, make_interval, timestampadd, timestampdiff, days, hours, minutes, months, years, from_utc_timestamp, to_utc_timestamp, convert_timezone, current_timezone, to_timestamp); fixtures `with_dayname`, `with_weekday`, `with_extract`, `with_unix_micros`, `make_timestamp_test`, `timestampadd_test`, `from_utc_timestamp_test`. **Phase 21** ‚úÖ **COMPLETED**: String (btrim, locate, conv), binary (hex, unhex, bin, getbit), type (to_char, to_varchar, to_number, try_to_number, try_to_timestamp), array (arrays_overlap, arrays_zip, explode_outer, posexplode_outer, array_agg), map (str_to_map), struct (transform_keys, transform_values). **Phase 20** ‚úÖ **COMPLETED**: Ordering (asc, desc, nulls_first/last), aggregates (median, mode, stddev_pop, var_pop, try_sum, try_avg), numeric (bround, negate, positive, cot, csc, sec, e, pi); fixtures `groupby_median`, `with_bround`; OrderBy supports optional nulls_first. **Phase 19** ‚úÖ **COMPLETED**: Aggregates (any_value, bool_and, bool_or, count_if, max_by, min_by, percentile, product, collect_list, collect_set), try_* (try_divide, try_add, try_subtract, try_multiply), misc (width_bucket, elt, bit_length, typeof); fixtures `groupby_any_value`, `groupby_product`, `try_divide`, `width_bucket`. **Phase 18** ‚úÖ **COMPLETED**: array/map/struct (map_filter, zip_with, map_zip_with). **Phase 17** ‚úÖ **COMPLETED**: Datetime/unix, math (pmod, factorial). **Phase 16** ‚úÖ **COMPLETED**: String/regex. **Phase 15** ‚úÖ **COMPLETED**: aliases, string, math, array_distinct. Remaining: ROADMAP Phases 25‚Äì26 (crate publish, Sparkless integration). **Phase 14**: Math (sin, cos, tan, asin, acos, atan, atan2, degrees, radians, signum), datetime (quarter, weekofyear, dayofweek, dayofyear, add_months, months_between, next_day), type/conditional (cast, try_cast, isnan, greatest, least); parity parser extended; fixtures `math_sin_cos`, `datetime_quarter_week`. **Phase 13**: String/binary/collection batch 1: ascii, format_number, overlay, position, char, chr, base64, unbase64, sha1, sha2, md5, array_compact implemented in Rust; parity parser and fixtures `string_ascii`, `string_format_number`. **Phase 12**: DataFrame methods implemented in Rust and exposed in Python: sample, random_split, first, head, tail, take, is_empty, to_json, to_pandas, explain, print_schema, checkpoint, repartition, coalesce, offset, summary, to_df, select_expr, col_regex, with_columns, with_columns_renamed, stat (cov/corr), na (fill/drop), freq_items, approx_quantile, crosstab, melt, except_all, intersect_all, sample_by, and Spark no-ops. Parity fixtures for first/head/offset: `first_row`, `head_n`, `offset_n`. **Phase 11**: Parity harness supports date, timestamp, and boolean in fixture input; datetime fixtures `date_add_sub`, `datediff`, `datetime_hour_minute`; String 6.4 fixtures `string_soundex`, `string_levenshtein`, `string_crc32`, `string_xxhash64`. Window fixtures percent_rank, cume_dist, ntile, nth_value are covered (multi-step workaround in harness). Phase 6: array functions `array_position`, `array_remove`, `posexplode` are **implemented** (via Polars list.eval); array fixtures `array_contains`, `element_at`, `array_size`, `array_sum`; array extensions (exists, forall, filter, transform, array_sum, array_mean; **Phase 8**: array_flatten, array_repeat **implemented** via map UDFs). **Phase 8**: Map (create_map, map_keys, map_values, map_entries, map_from_arrays **implemented**; Map as List(Struct{key, value})). JSON (get_json_object, from_json, to_json implemented). CI runs format, clippy, audit, deny, and all tests (including parity). Python smoke tests in `tests/python/` (run via `make test` or `make test-python`); see [PYTHON_API.md](PYTHON_API.md).

## Legend

- **‚úÖ Covered**: Covered by one or more fixtures (listed)
- **üöß Not yet covered**: Supported/partially supported but missing fixture coverage
- **‚ùå Not implemented**: Not implemented in the Rust API yet
- **‚ö†Ô∏è Diverges**: Implemented but intentionally differs from PySpark (must be documented)

## Coverage Matrix (high level)

| Area | Capability | Status | Fixtures |
| --- | --- | --- | --- |
| Data creation | `SparkSession::create_dataframe` (simple rows) | ‚úÖ Covered | `filter_age_gt_30`, `groupby_count`, `groupby_with_nulls` (and most others) |
| IO | `read_csv` | ‚úÖ Covered | `read_csv` |
| IO | `read_parquet` | ‚úÖ Covered | `read_parquet` |
| IO | `read_json` | ‚úÖ Covered | `read_json` |
| DataFrame | `select` | ‚úÖ Covered | many (e.g. `filter_age_gt_30`) |
| DataFrame | `filter` basic comparisons | ‚úÖ Covered | `filter_age_gt_30` |
| DataFrame | `filter` nested boolean logic | ‚úÖ Covered | `filter_and_or`, `filter_nested`, `filter_not` |
| DataFrame | `orderBy` | ‚úÖ Covered | many (e.g. `filter_age_gt_30`, `groupby_count`) |
| GroupBy | `groupBy(...).count()` | ‚úÖ Covered | `groupby_count`, `groupby_with_nulls` |
| GroupBy | `groupBy(...).sum()` | ‚úÖ Covered | `groupby_sum` |
| GroupBy | `groupBy(...).avg()` | ‚úÖ Covered | `groupby_avg` |
| GroupBy | `groupBy(...).min()` | ‚úÖ Covered | `groupby_min` |
| GroupBy | `groupBy(...).max()` | ‚úÖ Covered | `groupby_max` |
| GroupBy | groupBy with NULL keys | ‚úÖ Covered | `groupby_null_keys` |
| GroupBy | groupBy single-row groups / single group | ‚úÖ Covered | `groupby_single_row_groups`, `groupby_single_group` |
| GroupBy | multi-agg `agg([..])` | ‚úÖ Covered | `groupby_multi_agg` |
| GroupBy | stddev, variance, count_distinct in agg | ‚úÖ Covered | `groupby_stddev_count_distinct` |
| DataFrame | `withColumn` (arithmetic) | ‚úÖ Covered | `type_coercion_mixed` |
| DataFrame | `withColumn` (logical/boolean) | ‚úÖ Covered | `with_logical_column` |
| DataFrame | `withColumn` (mixed arithmetic + comparison) | ‚úÖ Covered | `with_arithmetic_logical_mix` |
| Functions | `when().then().otherwise()` | ‚úÖ Covered | `when_otherwise`, `when_then_otherwise` |
| Functions | `coalesce()` | ‚úÖ Covered | `coalesce` |
| Null semantics | NULL equality/inequality | ‚úÖ Covered | `null_comparison_equality` |
| Null semantics | NULL ordering comparisons | ‚úÖ Covered | `null_comparison_ordering` |
| Null semantics | `eqNullSafe` | ‚úÖ Covered | `null_safe_equality` |
| Null semantics | NULLs inside filter predicates | ‚úÖ Covered | `null_in_filter` |
| Type coercion | numeric comparison coercion (int vs double) | ‚úÖ Covered | `type_coercion_numeric` |
| Type coercion | numeric arithmetic coercion (int + double) | ‚úÖ Covered | `type_coercion_mixed` |
| Joins | inner/left/right/outer joins | ‚úÖ Covered | `inner_join`, `left_join`, `right_join`, `outer_join` |
| Joins | join with NULL keys (inner: nulls excluded) | ‚úÖ Covered | `join_null_keys` |
| Joins | join with duplicate keys (cartesian match) | ‚úÖ Covered | `join_duplicate_keys` |
| Windows | row_number, rank, dense_rank, lag, lead | ‚úÖ Covered | `row_number_window`, `rank_window`, `lag_lead_window` |
| Strings | upper, lower, substring, concat, concat_ws | ‚úÖ Covered | `string_upper_lower`, `string_substring`, `string_concat` |
| Strings | length, trim, ltrim, rtrim, regexp_extract, regexp_replace, split, initcap | ‚úÖ Covered | `string_length_trim` |
| Config | `spark.sql.caseSensitive` (case-insensitive column resolution) | ‚úÖ Covered | `case_insensitive_columns` |
| DataFrame | `union` / `unionAll` | ‚úÖ Covered | `union_all` |
| DataFrame | `unionByName` | ‚úÖ Covered | `union_by_name` |
| DataFrame | `distinct` / `dropDuplicates` | ‚úÖ Covered | `distinct` |
| DataFrame | `drop` (columns) | ‚úÖ Covered | `drop_columns` |
| DataFrame | `dropna` | ‚úÖ Covered | `dropna` |
| DataFrame | `fillna` (single value) | ‚úÖ Covered | `fillna` |
| DataFrame | `limit` | ‚úÖ Covered | `limit` |
| DataFrame | `withColumnRenamed` | ‚úÖ Covered | `with_column_renamed` |
| Array/List | array, array_contains, element_at, size/array_size, array_join, array_sort, array_slice, explode; array_position, array_remove, posexplode (implemented) | ‚úÖ Covered | `array_contains`, `element_at`, `array_size` |
| Windows | first_value, last_value, percent_rank | ‚úÖ Covered | `first_value_window`, `last_value_window`, `percent_rank_window` |
| Windows | cume_dist, ntile, nth_value | ‚úÖ Covered | `cume_dist_window`, `ntile_window`, `nth_value_window` (multi-step workaround in harness) |
| Strings | regexp_extract_all, regexp_like | ‚úÖ Covered | `regexp_extract_all`, `regexp_like` |
| Strings | repeat, reverse, instr, lpad, rpad | ‚úÖ Covered | `string_repeat_reverse`, `string_lpad_rpad` |
| Strings | mask, translate, substring_index; soundex, levenshtein, crc32, xxhash64 (Phase 8) | ‚úÖ Covered | `string_mask`, `string_translate`, `string_substring_index`, `string_soundex`, `string_levenshtein`, `string_crc32`, `string_xxhash64` |
| Strings (Phase 13) | ascii, format_number, overlay, position, char, chr, base64, unbase64, sha1, sha2, md5 | ‚úÖ Implemented | `string_ascii`, `string_format_number` |
| Strings (Phase 16) | regexp_count, regexp_instr, regexp_substr, split_part, find_in_set, format_string, printf | ‚úÖ Covered | `regexp_count`, `regexp_substr`, `regexp_instr`, `split_part`, `find_in_set`, `format_string` |
| Datetime (Phase 17) | unix_timestamp, from_unixtime, make_date, timestamp_seconds/millis/micros, unix_date, date_from_unix_date | ‚úÖ Covered | `unix_timestamp`, `from_unixtime`, `make_date`, `timestamp_seconds`, `timestamp_millis`, `timestamp_micros`, `unix_date`, `date_from_unix_date` |
| Math (Phase 17) | pmod, factorial | ‚úÖ Covered | `pmod`, `factorial` |
| Array | array_sum, array_exists, forall, filter, transform; array_flatten, array_repeat (Phase 8); array_compact (Phase 13) | ‚úÖ Implemented | `array_sum` |
| Map | create_map, map_keys, map_values, map_entries, map_from_arrays (Phase 8) | ‚úÖ Implemented | No fixture yet |
| JSON | get_json_object, from_json, to_json (Phase 10) | ‚úÖ get_json_object covered | `json_get_json_object` |
| Math | sqrt, pow, exp, log | ‚úÖ Covered | `math_sqrt_pow` |
| GroupBy | first, last, approx_count_distinct in agg | ‚úÖ Covered | `groupby_first_last` |
| GroupBy (Phase 19) | any_value, bool_and, bool_or, product, collect_list, collect_set, count_if, percentile, max_by, min_by | ‚úÖ Covered | `groupby_any_value`, `groupby_product` |
| Misc (Phase 19) | try_divide, try_add, try_subtract, try_multiply, width_bucket, elt, bit_length, typeof | ‚úÖ Covered | `try_divide`, `width_bucket` |
| DataFrame | replace, crossJoin, describe, subtract, intersect | ‚úÖ Covered | `replace`, `cross_join`, `describe`, `subtract`, `intersect` |
| SQL | `SparkSession::sql()` (optional `sql` feature) | ‚úÖ Implemented | No fixture (SQL translated to DataFrame ops; parity via DataFrame fixtures) |
| Datetime | year, month, day, to_date, date_format; current_date, date_add, hour, etc. | ‚úÖ Covered | `date_add_sub`, `datediff`, `datetime_hour_minute` |
| DataFrame (Phase 12) | first, head, offset, sample, to_json, summary, stat, select_expr, freq_items, crosstab, melt, etc. (Rust + PyO3) | ‚úÖ first/head/offset/summary covered | `first_row`, `head_n`, `offset_n`, `summary`; additional Phase 12 ops implemented, fixtures TBD |

## Fixture Index

| Fixture | What it covers |
| --- | --- |
| `filter_age_gt_30` | Filter + select + orderBy (baseline) |
| `filter_and_or` | AND/OR precedence + parentheses |
| `filter_nested` | Nested boolean logic |
| `filter_not` | NOT / negation |
| `groupby_count` | groupBy + count + orderBy |
| `groupby_with_nulls` | groupBy with NULLs |
| `groupby_sum` | groupBy + sum |
| `groupby_avg` | groupBy + avg |
| `groupby_min` | groupBy + min |
| `groupby_max` | groupBy + max |
| `groupby_null_keys` | groupBy with NULL keys |
| `groupby_single_row_groups` | groupBy with single-row groups (each key once) |
| `groupby_single_group` | groupBy with single group (all same key) |
| `join_null_keys` | inner join with NULL join keys (nulls excluded) |
| `join_duplicate_keys` | inner join with duplicate keys (multiple matches) |
| `case_insensitive_columns` | case-insensitive column resolution (filter/select/orderBy with mixed-case names) |
| `read_csv` | CSV read path + operations |
| `read_parquet` | Parquet read path + operations |
| `read_json` | JSON read path + operations |
| `with_logical_column` | Logical columns/expressions in withColumn |
| `with_arithmetic_logical_mix` | Mixed arithmetic + comparison in withColumn |
| `when_otherwise` | when/then/otherwise |
| `when_then_otherwise` | chained when |
| `coalesce` | coalesce null handling |
| `null_comparison_equality` | NULL equality/inequality semantics |
| `null_comparison_ordering` | NULL ordering semantics |
| `null_safe_equality` | eqNullSafe semantics |
| `null_in_filter` | NULLs in filter predicates |
| `type_coercion_numeric` | int/double comparison coercion |
| `type_coercion_mixed` | int+double arithmetic coercion |
| `inner_join` | inner join on dept_id |
| `left_join` | left join + orderBy |
| `right_join` | right join + orderBy |
| `outer_join` | outer join + orderBy |
| `groupby_multi_agg` | groupBy + multiple aggregations in one agg() |
| `groupby_stddev_count_distinct` | groupBy + stddev and count_distinct in agg |
| `row_number_window` | row_number() over partition by dept order by salary desc |
| `rank_window` | rank() over partition with ties |
| `lag_lead_window` | lag and lead over partition |
| `string_upper_lower` | upper(), lower() |
| `string_substring` | substring() 1-based |
| `string_concat` | concat(), concat_ws() |
| `string_length_trim` | length(), trim() in withColumn |
| `union_all` | union (vertical stack, same schema) |
| `union_by_name` | unionByName (align columns by name) |
| `distinct` | distinct (drop duplicate rows) |
| `drop_columns` | drop(columns) |
| `dropna` | dropna (drop rows with nulls) |
| `fillna` | fillna (fill nulls with value) |
| `limit` | limit(n) |
| `with_column_renamed` | withColumnRenamed(old, new) |
| `array_contains` | split + array_contains(col, lit) |
| `element_at` | split + element_at(col, 1-based index) |
| `array_size` | split + size(col) |
| `first_value_window` | first_value over partition |
| `last_value_window` | last_value over partition |
| `percent_rank_window` | percent_rank over partition |
| `cume_dist_window` | cume_dist over partition |
| `ntile_window` | ntile(n) over partition |
| `nth_value_window` | nth_value over partition |
| `regexp_like` | regexp_like(col, pattern) boolean match |
| `regexp_extract_all` | regexp_extract_all(col, pattern) list of matches |
| `string_repeat_reverse` | repeat(col, n), reverse(col) |
| `string_lpad_rpad` | lpad(col, len, pad), rpad(col, len, pad) |
| `math_sqrt_pow` | sqrt(col), pow(col, exp) |
| `groupby_first_last` | groupBy + first(name), last(name) |
| `groupby_any_value` | groupBy + any_value(column) |
| `groupby_product` | groupBy + product(column) |
| `try_divide` | try_divide(col, col) ‚Äî null on divide-by-zero |
| `width_bucket` | width_bucket(value, min, max, num_bucket) |
| `cross_join` | crossJoin (cartesian product) |
| `describe` | describe() summary statistics |
| `summary` | summary() (same as describe) |
| `replace` | replace(column, old_value, new_value) |
| `subtract` | subtract (set difference) |
| `intersect` | intersect (set intersection) |
| `first_row` | first() ‚Äì first row as one-row DataFrame |
| `head_n` | head(n) ‚Äì first n rows |
| `offset_n` | offset(n) ‚Äì skip first n rows |
| `string_mask` | mask(col) ‚Äì replace upper/lower/digit with X/x/n |
| `string_translate` | translate(col, from_str, to_str) |
| `string_substring_index` | substring_index(col, delim, count) before/after nth delim |
| `array_sum` | array(cols) + array_sum(col) |
| `json_get_json_object` | get_json_object(col, '$.path') |
| `date_add_sub` | date_add(col('d'), 7), date_sub(col('d'), 3) |
| `datediff` | datediff(col('end'), col('start')) |
| `datetime_hour_minute` | hour(col('ts')), minute(col('ts')) with timestamp input |
| `string_soundex` | soundex(col('name')) |
| `string_levenshtein` | levenshtein(col('a'), col('b')) |
| `string_crc32` | crc32(col('s')) |
| `string_xxhash64` | xxhash64(col('s')) |
| `string_ascii` | ascii(col('name')) ‚Üí first-char code point |
| `string_format_number` | format_number(col('value'), 2) ‚Üí fixed-decimal string |
| `phase15_aliases_nvl_isnull` | nvl, nvl2, isnull, isnotnull (Phase 15) |
| `string_left_right_replace` | left, right, replace, startswith, endswith, contains, like, ilike, rlike |
| `math_cosh_cbrt` | cosh, sinh, tanh, acosh, asinh, atanh, cbrt, expm1, log1p, log10, log2, rint, hypot |
| `array_distinct` | array_distinct(col) ‚Äî *skipped* (order divergence) |
| `regexp_count` | regexp_count(col, pattern) ‚Äì count non-overlapping matches |
| `regexp_substr` | regexp_substr(col, pattern) ‚Äì first match substring |
| `regexp_instr` | regexp_instr(col, pattern) ‚Äì 1-based position of first match |
| `split_part` | split_part(col, delim, part_num) ‚Äì 1-based part of split |
| `find_in_set` | find_in_set(col('str'), col('set')) ‚Äì 1-based index in comma-delimited list |
| `format_string` | format_string('%d %s', col('a'), col('b')) ‚Äì printf-style formatting |
| `unix_timestamp` | unix_timestamp(col), unix_timestamp(col, format) ‚Äì string to seconds |
| `from_unixtime` | from_unixtime(col), from_unixtime(col, format) ‚Äì seconds to formatted string |
| `make_date` | make_date(year, month, day) ‚Äì build date from parts |
| `timestamp_seconds` | timestamp_seconds(col) ‚Äì seconds epoch to timestamp |
| `timestamp_millis` | timestamp_millis(col) ‚Äì millis epoch to timestamp |
| `timestamp_micros` | timestamp_micros(col) ‚Äì micros epoch to timestamp |
| `unix_date` | unix_date(col) ‚Äì date to days since epoch |
| `date_from_unix_date` | date_from_unix_date(col) ‚Äì days to date |
| `pmod` | pmod(a, b) ‚Äì positive modulus |
| `factorial` | factorial(n) ‚Äì n! for n 0..20 |
| `with_bit_ops` | bit operations (bit_and, bit_or, bit_xor, bit_count, bit_get) via withColumn |

## Next additions to the matrix (recommended)

- Add more join edge-case fixtures (e.g. left/outer with null keys) if needed.
- **ROADMAP Phases 16‚Äì26**: Phases 18‚Äì19 completed. Phases 20‚Äì24 (full parity in 5 parts), Phase 25 (publish crate on crates.io), Phase 26 (Sparkless integration, 200+ tests). See [ROADMAP.md](ROADMAP.md), [GAP_ANALYSIS_SPARKLESS_3.28.md](GAP_ANALYSIS_SPARKLESS_3.28.md).

## Sparkless Test Conversion

Sparkless ([github.com/eddiethedean/sparkless](https://github.com/eddiethedean/sparkless)) has 270+ JSON expected outputs in `tests/expected_outputs/`. These can drive robin-sparkless parity tests via a fixture converter that maps Sparkless JSON format ‚Üí robin-sparkless fixture format. See [SPARKLESS_INTEGRATION_ANALYSIS.md](SPARKLESS_INTEGRATION_ANALYSIS.md) ¬ß4 for:
- Fixture format comparison (input_data vs input/rows; expected_output vs expected)
- Conversion steps per test
- Priority order: parity/dataframe, parity/functions, then parity/sql

