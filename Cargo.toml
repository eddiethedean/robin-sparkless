[workspace]
members = ["node"]

[package]
name = "robin-sparkless"
version = "0.1.0"
edition = "2021"
description = "PySpark-like DataFrame API in Rust on Polars; no JVM."
license = "MIT"
repository = "https://github.com/eddiethedean/robin-sparkless"
keywords = ["pyspark", "polars", "dataframe", "spark", "etl"]
categories = ["data-structures", "development-tools"]
authors = ["Robin Sparkless contributors"]

[lib]
name = "robin_sparkless"
# When building the Python wheel, maturin adds cdylib and sets extension-module linker flags.
crate-type = ["rlib"]

[features]
default = []
pyo3 = ["dep:pyo3"]
sql = ["dep:sqlparser"]
delta = ["dep:deltalake", "dep:tokio"]

[dependencies]
polars = { version = "0.45", features = ["lazy", "csv", "parquet", "json", "temporal", "strings", "concat_str", "regex", "round_series", "abs", "dtype-categorical", "dtype-date", "dtype-datetime", "dtype-duration", "dtype-time", "rank", "is_in", "list_eval", "list_drop_nulls", "list_any_all", "cum_agg", "string_pad", "string_reverse", "repeat_by", "log", "month_end", "describe", "cross_join", "semi_anti_join", "extract_jsonpath", "dtype-struct", "random", "product", "moment"] }
serde = { version = "1.0", features = ["derive"] }
chrono = "0.4"
chrono-tz = "0.9"
serde_json = "1.0"
anyhow = "1.0"
thiserror = "1.0"
pyo3 = { version = "0.24", optional = true, features = ["extension-module"] }
sqlparser = { version = "0.45", optional = true }
deltalake = { version = "0.30", optional = true }
tokio = { version = "1", optional = true, features = ["rt", "rt-multi-thread", "fs", "io-util"] }
url = "2"
# Phase 23: URL encode/decode
percent-encoding = "2"
# Phase 8: string UDFs and list helpers (used in map closures)
strsim = "0.11"
crc32fast = "1.4"
twox-hash = "1.6"
soundex = "0.2"
# Pin to address RUSTSEC-2026-0007 (integer overflow in BytesMut::reserve; transitive via polars/deltalake)
bytes = "1.11.1"
# Phase 12: colRegex (column name pattern matching)
regex = "1.11"
# Phase 13: base64, binary hashes; Phase 2: encode/decode
base64 = "0.22"
hex = "0.4"
sha1 = "0.10"
sha2 = "0.10"
md5 = "0.7"
# Phase 24: rand/randn with seed support
rand = "0.8"
rand_distr = "0.4"
# Phase 2: AES crypto (PySpark aes_encrypt/aes_decrypt)
aes-gcm = "0.10"

[dev-dependencies]
chrono = "0.4"
polars = { version = "0.45", features = ["lazy", "csv", "parquet", "json", "temporal", "strings", "concat_str", "regex", "round_series", "abs", "dtype-categorical", "dtype-date", "dtype-datetime", "dtype-duration", "dtype-time", "test", "rank", "is_in", "list_eval", "list_drop_nulls", "list_any_all", "cum_agg", "string_pad", "string_reverse", "repeat_by", "log", "month_end", "describe", "cross_join", "semi_anti_join", "extract_jsonpath", "dtype-struct", "product", "moment"] }
tempfile = "3"
criterion = "0.5"

[[bench]]
name = "filter_select_groupby"
harness = false
